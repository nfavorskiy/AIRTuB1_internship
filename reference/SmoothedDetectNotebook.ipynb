{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Detection' or 'Classification'\n",
    "prediction_strategy = 'Detection'\n",
    "\n",
    "# 'Raw', 'Smoothed' or 'Smoothed and Alligned'\n",
    "data_prep_stage = 'Smoothed'\n",
    "\n",
    "# 'Enabled' or 'Disabled'\n",
    "postmodeling_renders = 'Disabled'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from plyfile import PlyData\n",
    "\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the GPU memory growth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scans with annotations and, per preparation stage, filepaths\n",
    "scans = {}\n",
    "\n",
    "# render settings per data preparation stage \n",
    "stage_specific_settings = {\n",
    "    'Raw': {\n",
    "        'depth': (50, 400),\n",
    "    },\n",
    "    'Smoothed': {\n",
    "        'depth': (-0.5, 0.5),\n",
    "    },\n",
    "    'Smoothed and Alligned': {\n",
    "        'depth': (-0.5, 0.5),\n",
    "    },\n",
    "}\n",
    "\n",
    "# set default settings based on the chosen data preparation stage  \n",
    "default_settings = stage_specific_settings[data_prep_stage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 221111_144114__145900to148050 - First scan ever used\n",
    "# flightpath: from class 3 to class 2\n",
    "\n",
    "scans['221111_144114__145900to148050'] = {\n",
    "        'Raw':                      'volume/scanData/221111_144114__binary_onlylines145900to148050.ply',\n",
    "        'Smoothed':                 'volume/scanData/6_flight_vibr_orig_binary.ply',\n",
    "        'Smoothed and Alligned':    'volume/scanData/6_flight_comp_vibr_binary.ply',\n",
    "        'Annotations': {\n",
    "            (280, 560): 3,\n",
    "            (590, 640): 2,\n",
    "            (660, 700): 2,\n",
    "            (740, 810): 2,\n",
    "            (960, 1010): 2,\n",
    "            (1140, 1200): 2,\n",
    "            (1230, 1280): 2,\n",
    "            (1380, 1400): 2,\n",
    "            (1450, 1470): 2,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 221111_144020__11170to15130 - Unused due to high distortion\n",
    "# flightpath: from class 3 to class 2\n",
    "\n",
    "# scans['221111_144020__11170to15130'] = {\n",
    "#     'Raw':                      'volume/scanData/221111_144020__binary_onlylines11170to15130raw.ply',\n",
    "#     'Smoothed':                 'volume/scanData/221111_144020__binary_onlylines11170to15130smoothedbySLM.ply',\n",
    "#     'Smoothed and Alligned':    'volume/scanData/221111_144020__binary_onlylines11170to15130smoothedbySLM_AL.ply',\n",
    "#     'Annotations': {\n",
    "#         (980, 1020): 2,\n",
    "#         (1040, 1080): 2,\n",
    "#         (1180, 1220): 2,\n",
    "#         (1460, 1500): 2,\n",
    "#         (1520, 1580): 2,\n",
    "#         (1620, 1860): 3,\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 221111_144114__40900to45309 - Unused due to high distortion\n",
    "# flightpath: from class 3 to class 2\n",
    "\n",
    "# scans['221111_144114__40900to45309'] = {\n",
    "#     'Raw':                      'volume/scanData/221111_144020__binary_onlylines40900to45309raw.ply',\n",
    "#     'Smoothed':                 'volume/scanData/221111_144020__binary_onlylines40900to45309smoothedbySLM.ply',\n",
    "#     'Smoothed and Alligned':    'volume/scanData/221111_144020__binary_onlylines40900to45309smoothedbySLM_AL.ply',\n",
    "#     'Annotations': {\n",
    "#         (200, 480): 3,\n",
    "#         (520, 640): 2,\n",
    "#         (660, 720): 2,\n",
    "#         (920, 960): 2,\n",
    "#         (1000, 1020): 2,\n",
    "#         (1100, 1140): 2,\n",
    "#         (1180, 1240): 2,\n",
    "#         (1500, 1560): 2,\n",
    "#     },\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 221111_144114__25660to28100 - A 2nd iteration scan - recently added\n",
    "# flightpath: from class 3 to class 2\n",
    "\n",
    "scans['221111_144114__25660to28100'] = {\n",
    "        'Raw':                      'volume/scanData/',\n",
    "        'Smoothed':                 'volume/scanData/221111_144114__binary_onlylines25660to28100 rloess 0.09.ply',\n",
    "        'Smoothed and Alligned':    'volume/scanData/',\n",
    "        'Annotations': {\n",
    "            (200, 530): 3,\n",
    "            (590, 660): 2,\n",
    "            (680, 740): 2,\n",
    "            (790, 880): 2,\n",
    "            (1080, 1110): 2,\n",
    "            (1270, 1330): 2,\n",
    "            (1380, 1430): 2,\n",
    "            (1560, 1580): 2,\n",
    "            (1660, 1700): 2,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 221111_144114__47178to50517 - A 2nd iteration scan - recently added\n",
    "# flightpath: from class 2 to class 3\n",
    "\n",
    "scans['221111_144114__47178to50517'] = {\n",
    "        'Raw':                      'volume/scanData/',\n",
    "        'Smoothed':                 'volume/scanData/221111_144114__binary_onlylines47178to50517 rloess 0.09.ply',\n",
    "        'Smoothed and Alligned':    'volume/scanData/',\n",
    "        'Annotations': {\n",
    "            (2550, 3060): 3,\n",
    "            (2400, 2480): 2,\n",
    "            (2350, 2370): 2,\n",
    "            (2270, 2300): 2,\n",
    "            (1860, 1900): 2,\n",
    "            (1540, 1620): 2,\n",
    "            (1400, 1450): 2,\n",
    "            (1170, 1200): 2,\n",
    "            (1070, 1100): 2,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 221111_144114__86841to90142 - A 2nd iteration scan - recently added\n",
    "# flightpath: from class 3 to class 2\n",
    "\n",
    "scans['221111_144114__86841to90142'] = {\n",
    "        'Raw':                      'volume/scanData/',\n",
    "        'Smoothed':                 'volume/scanData/221111_144114__binary_onlylines86841to90142 rloess 0.09.ply',\n",
    "        'Smoothed and Alligned':    'volume/scanData/',\n",
    "        'Annotations': {\n",
    "            (240, 640): 3,\n",
    "            (730, 800): 2,\n",
    "            (840, 910): 2,\n",
    "            (950, 1080): 2,\n",
    "            (1340, 1420): 2,\n",
    "            (1670, 1740): 2,\n",
    "            (1820, 1890): 2,\n",
    "            (2130, 2170): 2,\n",
    "            (2240, 2270): 2,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 221111_144114__114378to117271 - A 2nd iteration scan - recently added\n",
    "# flightpath: from class 2 to class 3\n",
    "\n",
    "scans['221111_144114__114378to117271'] = {\n",
    "        'Raw':                      'volume/scanData/',\n",
    "        'Smoothed':                 'volume/scanData/221111_144114__binary_onlylines114378to117271 rloess 0.09.ply',\n",
    "        'Smoothed and Alligned':    'volume/scanData/',\n",
    "        'Annotations': {\n",
    "            (2130, 2630): 3,\n",
    "            (1920, 2010): 2,\n",
    "            (1790, 1890): 2,\n",
    "            (1610, 1750): 2,\n",
    "            (1310, 1380): 2,\n",
    "            (1110, 1160): 2,\n",
    "            (1010, 1050): 2,\n",
    "            (860, 890): 2,\n",
    "            (770, 800): 2,\n",
    "        },\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get erosion class of a given profile with the offset in scan in mind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_erosion_class_based_on_x_coordinate(annotations_dict, x):\n",
    "    for annotation in annotations_dict.keys():\n",
    "        if x > annotation[0] and x < annotation[1]:\n",
    "            if prediction_strategy == 'Detection':\n",
    "                return 1 \n",
    "            elif prediction_strategy == 'Classification': \n",
    "                return annotations_dict[annotation]\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to make cute annotation lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_line(ax, xmin, xmax, y, text, ytext = 0, linecolor = 'red', linewidth = 1, fontsize = 12):\n",
    "    ax.annotate('',\n",
    "                xy=(xmin, y),\n",
    "                xytext=(xmax, y),\n",
    "                xycoords='data',\n",
    "                textcoords='data',\n",
    "                arrowprops={'arrowstyle': '|-|',\n",
    "                            'color': linecolor, \n",
    "                            'linewidth':linewidth\n",
    "                           }\n",
    "               )\n",
    "    \n",
    "    ax.annotate('', \n",
    "                xy=(xmin, y), \n",
    "                xytext=(xmax, y), \n",
    "                xycoords='data', \n",
    "                textcoords='data',\n",
    "                arrowprops={'arrowstyle': '<->', \n",
    "                            'color': linecolor, \n",
    "                            'linewidth':linewidth\n",
    "                           }\n",
    "               )\n",
    "\n",
    "    xcenter = xmin + (xmax - xmin) / 2\n",
    "    \n",
    "    if ytext == 0:\n",
    "        ytext = y + (ax.get_ylim()[1] - ax.get_ylim()[0]) / 2\n",
    "\n",
    "    ax.annotate(text, xy=(xcenter,ytext), ha='center', va='center', fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data from scans alongside the filepaths in the dictionaries. All scans are made to start from 0 on the X axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scan_data in scans.values():\n",
    "    file = scan_data[data_prep_stage]\n",
    "    data = PlyData.read(file)\n",
    "\n",
    "    data['vertex']['x'] -= min(data['vertex']['x'])\n",
    "\n",
    "    # indices_to_remove = data['vertex']['z'] == 0\n",
    "    # # https://stackoverflow.com/a/24553551\n",
    "    # x = np.delete(data['vertex']['x'], indices_to_remove)\n",
    "    # y = np.delete(data['vertex']['y'], indices_to_remove)\n",
    "    # z = np.delete(data['vertex']['z'], indices_to_remove)\n",
    "\n",
    "    # data['vertex']['x'] = x\n",
    "    # data['vertex']['y'] = y\n",
    "    # data['vertex']['z'] = z\n",
    "\n",
    "    scan_data['data'] = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth_range = default_settings['depth']\n",
    "figs_and_plots = []\n",
    "plot_count = 0\n",
    "\n",
    "for scan_data in scans.values():\n",
    "    data = scan_data['data']\n",
    "\n",
    "    x = data['vertex']['x']\n",
    "    y = data['vertex']['y']\n",
    "    z = data['vertex']['z']\n",
    "\n",
    "    # print(len(np.unique(x)))\n",
    "\n",
    "    figs_and_plots.append(plt.subplots(1, figsize=(48,8)))\n",
    "    image = figs_and_plots[plot_count][1].scatter(x, y, c=z, s=0.1, vmin=depth_range[0], vmax=depth_range[1])\n",
    "    figs_and_plots[plot_count][1].title.set_text('Annotated erosion on blade')\n",
    "    figs_and_plots[plot_count][0].colorbar(image, ax=figs_and_plots[plot_count][1])\n",
    "    # plt.xticks(\n",
    "    #     ticks = np.arange(0, len(np.unique(x)), 20), \n",
    "    #     labels = list(map(int, np.unique(x)[::20])), \n",
    "    #     rotation = 90,\n",
    "    # )\n",
    "\n",
    "    for erosion_annotation in scan_data['Annotations'].keys():\n",
    "        erosion_class = get_erosion_class_based_on_x_coordinate(\n",
    "            scan_data['Annotations'], \n",
    "            erosion_annotation[0] + 1,\n",
    "        )\n",
    "        annotation_line(\n",
    "            ax = figs_and_plots[plot_count][1], \n",
    "            text=str(erosion_class), \n",
    "            xmin=erosion_annotation[0], \n",
    "            xmax=erosion_annotation[1], \n",
    "            y=0, \n",
    "            ytext=10, \n",
    "            linewidth=3, \n",
    "            linecolor='red', \n",
    "            fontsize=16\n",
    "        )\n",
    "    # fig, axs = plt.subplots(1, 3, figsize=(16,9))\n",
    "\n",
    "    # axs[0].boxplot(x)\n",
    "    # axs[0].set_title('X Axis')\n",
    "\n",
    "    # axs[1].boxplot(y)\n",
    "    # axs[1].set_title('Y Axis')\n",
    "\n",
    "    # axs[2].boxplot(z)\n",
    "    # axs[2].set_title('Z Axis')\n",
    "\n",
    "    plot_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find an object with most subobjects in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_length_list(l):\n",
    "    list_len = [len(i) for i in l]\n",
    "    return max(list_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to make all items the same length as the longest item in the list by filling it with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_to_length_with(a, N, x = 0):\n",
    "    return a + [x] * (N - len(a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to evaluate predictions of a neural network from a lists of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(nn_predictions):\n",
    "    nn_predictions = [np.argmax(pred) for pred in nn_predictions]\n",
    "    return nn_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to render Confusion Matrix and calculate Accuracy, Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_scores(test_labels, predicted_labels):\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predicted_labels)\n",
    "    precision = precision_score(test_labels, predicted_labels, labels=test_labels, average=\"micro\")\n",
    "    recall = recall_score(test_labels, predicted_labels, labels=test_labels, average=\"micro\")\n",
    "\n",
    "    # print(\"Accuracy is:\", accuracy)\n",
    "    # print(\"Accuracy is the ratio of correct True Positives and True negatives,\")\n",
    "    # print(\"This metric is always iffy since in our case it would score almost 40% by always saying that there is no erosion.\")\n",
    "    # print()\n",
    "\n",
    "    # print(\"Precision is:\", precision)\n",
    "    # print(\"Precision is the ratio of actual True Positives in the predicted Positives,\")\n",
    "    # print(\"In other words: how many items that we predicted to be a class are actually of that class?\")\n",
    "    # print()\n",
    "\n",
    "    # print(\"Recall is:\", recall)\n",
    "    # print(\"Recall is the ratio of predicted True Positives of all True Positives,\")\n",
    "    # print(\"In other words: how many of all the True Positives did we find?\")\n",
    "    # print()\n",
    "\n",
    "    # disp = ConfusionMatrixDisplay.from_predictions(test_labels, predicted_labels)\n",
    "    # disp.ax_.set_title(\"Predicted values vs actual values of erosion class\")\n",
    "\n",
    "    return accuracy, precision, recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to render predictions after modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postmodelling_render(model_type, fold, predictions):\n",
    "    if postmodeling_renders != \"Disabled\" and fold == 1:\n",
    "        for scan_data in scans.values():\n",
    "            fig, ax = plt.subplots(1, figsize=(48,8))\n",
    "            image = ax.scatter(x, y, c=z, s=0.1, vmin=depth_range[0], vmax=depth_range[1])\n",
    "            ax.title.set_text(data_prep_stage + ' blade - ' + model_type + ' - Erosion ' + prediction_strategy + ', Fold #' + str(fold))\n",
    "            fig.colorbar(image, ax=ax)\n",
    "            \n",
    "            for erosion_annotation in scan_data['Annotations'].keys():\n",
    "                erosion_class = get_erosion_class_based_on_x_coordinate(\n",
    "                    scan_data['Annotations'], \n",
    "                    erosion_annotation[0] + 1,\n",
    "                )\n",
    "                annotation_line(\n",
    "                    ax = ax, \n",
    "                    text='Class ' + str(erosion_class), \n",
    "                    xmin=erosion_annotation[0], \n",
    "                    xmax=erosion_annotation[1], \n",
    "                    y=0, \n",
    "                    ytext=10,\n",
    "                    linewidth=3, \n",
    "                    linecolor='red', \n",
    "                    fontsize=16\n",
    "                )\n",
    "            \n",
    "            for i in range(len(train_labels_ids)):\n",
    "                annotation_line(\n",
    "                    ax = ax, \n",
    "                    text='', \n",
    "                    xmin=train_labels_ids[i]*11, \n",
    "                    xmax=train_labels_ids[i]*11 + 10, \n",
    "                    y=-298, \n",
    "                    ytext=-280, \n",
    "                    linewidth=3, \n",
    "                    linecolor='yellow', \n",
    "                    fontsize=16\n",
    "                )\n",
    "            \n",
    "            for i in range(len(test_labels)):\n",
    "                if predictions[i] == test_labels[i]:\n",
    "                    annotation_line(\n",
    "                        ax = ax, \n",
    "                        text=predictions[i], \n",
    "                        xmin=test_labels_ids[i]*11, \n",
    "                        xmax=test_labels_ids[i]*11 + 10, \n",
    "                        y=-298, \n",
    "                        ytext=-280, \n",
    "                        linewidth=3, \n",
    "                        linecolor='green', \n",
    "                        fontsize=16\n",
    "                    )\n",
    "                \n",
    "                else:\n",
    "                    annotation_line(\n",
    "                        ax = ax, \n",
    "                        text=predictions[i], \n",
    "                        xmin=test_labels_ids[i]*11, \n",
    "                        xmax=test_labels_ids[i]*11 + 10, \n",
    "                        y=-298, \n",
    "                        ytext=-280, \n",
    "                        linewidth=3, \n",
    "                        linecolor='red', \n",
    "                        fontsize=16\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total number of profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(np.unique(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length_list = 0\n",
    "for scan_data in scans.values():\n",
    "    scan_data['slice_start_coordinates'] = []\n",
    "    scan_data['x_slices'] = []\n",
    "    scan_data['y_slices'] = []\n",
    "    scan_data['z_slices'] = []\n",
    "\n",
    "    current_x = 0\n",
    "\n",
    "    for xi in np.unique(scan_data['data']['vertex']['x'])[1:]:\n",
    "        if xi > current_x + 10:\n",
    "            current_x = xi\n",
    "            # print(xi)\n",
    "            scan_data['slice_start_coordinates'].append(xi)\n",
    "\n",
    "    print(\"There are\", len(scan_data['slice_start_coordinates']), \"slices to be made\")\n",
    "    print(scan_data['slice_start_coordinates'])\n",
    "\n",
    "    stop = scan_data['slice_start_coordinates'][-1]\n",
    "    current_index = 0\n",
    "\n",
    "    for coordinate in scan_data['slice_start_coordinates']:\n",
    "        if coordinate == stop:\n",
    "            break\n",
    "\n",
    "        next_coordinate = scan_data['slice_start_coordinates'][current_index + 1]\n",
    "        mask = (\n",
    "            (scan_data['data']['vertex']['x'] >= coordinate) &\n",
    "            (scan_data['data']['vertex']['x'] < next_coordinate)\n",
    "        )\n",
    "\n",
    "        scan_data['x_slices'].append(scan_data['data']['vertex']['x'][mask])\n",
    "        scan_data['y_slices'].append(scan_data['data']['vertex']['y'][mask])\n",
    "        scan_data['z_slices'].append(scan_data['data']['vertex']['z'][mask])\n",
    "        \n",
    "        current_index += 1\n",
    "\n",
    "    # scan_data['cnnz_profiles'] = []\n",
    "    # for profile_number in np.unique(scan_data['data']['vertex']['x']):\n",
    "    #     mask = scan_data['data']['vertex']['x'] == profile_number\n",
    "    #     scan_data['cnnz_profiles'].append(scan_data['data']['vertex']['z'][mask])\n",
    "\n",
    "    # print(scan_data['cnnz_profiles'].shape)\n",
    "\n",
    "    # scan_data['cnnz_profiles'] = np.array(scan_data['cnnz_profiles'])\n",
    "    # scan_data['cnnz_profiles'] = np.array_split(scan_data['cnnz_profiles'], len(scan_data['slice_start_coordinates']))\n",
    "    # scan_data['cnnz_profiles'] = np.array(scan_data['cnnz_profiles'])\n",
    " \n",
    "    max_length_list = max(max_length_list, find_max_length_list(scan_data['z_slices']))\n",
    "    # max_length_list = max(max_length_list, find_max_length_list(scan_data['cnnz_profiles']))\n",
    "\n",
    "    # shape = scan_data['cnnz_profiles'].shape\n",
    "    # scan_data['cnnz_profiles'] = scan_data['cnnz_profiles'].reshape((shape[0], shape[1], shape[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scan_data in scans.values():\n",
    "    scan_data['z_slices_padded'] = []\n",
    "    for z_slice in scan_data['z_slices']:\n",
    "        scan_data['z_slices_padded'].append(pad_list_to_length_with(z_slice.tolist(), max_length_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing mean erosion of each slice as future target for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scan_data in scans.values():\n",
    "    scan_data['targets'] = []\n",
    "    for x_slice in scan_data['x_slices']:\n",
    "        scan_data['targets'].append(get_erosion_class_based_on_x_coordinate(\n",
    "            scan_data['Annotations'],\n",
    "            mean(x_slice),\n",
    "        ))\n",
    "    print(scan_data['targets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding each slice label with the number of that slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for scan_data in scans.values():\n",
    "    for i in range(len(scan_data['targets'])):\n",
    "        scan_data['targets'][i] = scan_data['targets'][i] + i * 10\n",
    "\n",
    "    print(scan_data['targets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge of slices and tagets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_slices_padded = []\n",
    "targets = []\n",
    "for scan_data in scans.values():\n",
    "    for slice in scan_data['z_slices_padded']:\n",
    "        z_slices_padded.append(slice)\n",
    "    for target in scan_data['targets']:\n",
    "        targets.append(target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data onto train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data, test_data, train_labels, test_labels = train_test_split(z_slices_padded, \n",
    "#                                                                     targets, \n",
    "#                                                                     test_size=0.33, \n",
    "#                                                                     #random_state=42\n",
    "#                                                                 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isolating slice numbers and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolate_slice_ids():\n",
    "    train_labels_ids = np.zeros(len(train_labels), dtype=int)\n",
    "\n",
    "    for i in range(len(train_labels)):\n",
    "        train_labels_ids[i] = train_labels[i] / 10\n",
    "        train_labels[i] = train_labels[i] % 10\n",
    "\n",
    "    test_labels_ids = np.zeros(len(test_labels), dtype=int)\n",
    "\n",
    "    for i in range(len(test_labels)):\n",
    "        test_labels_ids[i] = test_labels[i] / 10\n",
    "        test_labels[i] = test_labels[i] % 10\n",
    "    \n",
    "    return train_labels, test_labels, train_labels_ids, test_labels_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling():\n",
    "    # min_max_scaler = MinMaxScaler()\n",
    "    # min_max_scaler.fit(train_data)\n",
    "    # train_data = min_max_scaler.transform(train_data)\n",
    "    # test_data = min_max_scaler.transform(test_data)\n",
    "\n",
    "    standard_scaler = StandardScaler()\n",
    "    standard_scaler.fit(train_data)\n",
    "\n",
    "    global standard_scaled_train_data\n",
    "    standard_scaled_train_data = standard_scaler.transform(train_data)\n",
    "    \n",
    "    global standard_scaled_test_data\n",
    "    standard_scaled_test_data = standard_scaler.transform(test_data)\n",
    "\n",
    "    # type(train_data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snn1_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(512, activation='relu', input_shape=(max_length_list,)),\n",
    "        tf.keras.layers.Dense(4),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_snn2_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(512, activation='relu', input_shape=(max_length_list,)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn1_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        # tf.keras.layers.Lambda(lambda x: x, input_shape=(max_length_list, 10, )),\n",
    "        tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='linear', padding='same', input_shape=(max_length_list, 10)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Softmax()\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_metrics_scores(model, predictions):\n",
    "    metrics_scores[model] = tuple(map(sum, zip(\n",
    "        metrics_scores[model], \n",
    "        model_scores(test_labels, predictions,\n",
    "    ))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_scores = {\n",
    "    \"Zero Benchmark\":                       (0, 0, 0),\n",
    "    \"High Benchmark\":                       (0, 0, 0),\n",
    "    \"Logistic Regression\":                  (0, 0, 0),\n",
    "    \"Support Vector Machine\":               (0, 0, 0),\n",
    "    \"Random Forest\":                        (0, 0, 0),\n",
    "    \"Sequential Neural Network 1\":          (0, 0, 0),\n",
    "    \"Sequential Neural Network 2\":          (0, 0, 0),\n",
    "    \"Convolutional Neural Network 1\":       (0, 0, 0),\n",
    "}\n",
    "\n",
    "num_folds = 4\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "z_slices_padded = np.array(z_slices_padded)\n",
    "targets = np.array(targets)\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(kfold.split(z_slices_padded, targets)):\n",
    "\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    print(f\"train_indices type: {type(train_indices)}, val_indices type: {type(val_indices)}\")\n",
    "\n",
    "    train_data, test_data = z_slices_padded[train_indices], z_slices_padded[val_indices]\n",
    "    print(train_data.shape)\n",
    "\n",
    "    train_labels, test_labels = targets[train_indices], targets[val_indices]\n",
    "    \n",
    "    train_labels, test_labels, train_labels_ids, test_labels_ids = isolate_slice_ids()\n",
    "    scaling()\n",
    "\n",
    "    # Zero Benchmark\n",
    "    zero_benchmark = len(test_labels)*[0]\n",
    "    update_metrics_scores(\"Zero Benchmark\", zero_benchmark)\n",
    "    # postmodelling_render('Zero Benchmark', fold, zero_benchmark)\n",
    "\n",
    "    # High Benchmark\n",
    "    if prediction_strategy == 'Detection':\n",
    "        top_class = 1\n",
    "    elif prediction_strategy == 'Classification':\n",
    "        top_class = 3\n",
    "    high_benchmark = len(test_labels)*[top_class]\n",
    "    update_metrics_scores(\"High Benchmark\", high_benchmark)\n",
    "    # postmodelling_render('High Benchmark', fold, high_benchmark)\n",
    "\n",
    "    # Logistic Regression\n",
    "    lr = LogisticRegression(random_state=0, max_iter=1000).fit(standard_scaled_train_data, train_labels)\n",
    "    lr_predictions = lr.predict(standard_scaled_test_data)\n",
    "    update_metrics_scores(\"Logistic Regression\", lr_predictions)\n",
    "    # postmodelling_render('Logistic Regression', fold, lr_predictions)\n",
    "\n",
    "    # Support Vector Machine\n",
    "    svc = svm.SVC(random_state=0, kernel=\"linear\").fit(standard_scaled_train_data, train_labels)\n",
    "    svc_predictions = svc.predict(standard_scaled_test_data)\n",
    "    update_metrics_scores(\"Support Vector Machine\", svc_predictions)\n",
    "    # postmodelling_render('Support Vector Machine', fold, svc_predictions)\n",
    "\n",
    "    # Random Forest\n",
    "    rf = RandomForestClassifier().fit(train_data, train_labels)\n",
    "    rf_predictions = rf.predict(test_data)\n",
    "    update_metrics_scores(\"Random Forest\", rf_predictions)\n",
    "    postmodelling_render('Random Forest', fold, rf_predictions)\n",
    "\n",
    "    # Sequential Neural Network 1\n",
    "    model1 = create_snn1_model()\n",
    "    model1.compile(optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    model1.fit(train_data, train_labels, epochs=10)\n",
    "    nn_predictions1 = model1.predict(test_data)\n",
    "    nn_predictions1 = evaluate_predictions(nn_predictions1)\n",
    "    update_metrics_scores(\"Sequential Neural Network 1\", nn_predictions1)\n",
    "    postmodelling_render('Sequential Neural Network 1', fold, nn_predictions1)\n",
    "\n",
    "    # Sequential Neural Network 2\n",
    "    model2 = create_snn2_model()\n",
    "    model2.compile(optimizer='adam',\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    model2.fit(train_data, train_labels, epochs=10)\n",
    "    nn_predictions2 = model2.predict(test_data)\n",
    "    nn_predictions2 = evaluate_predictions(nn_predictions2)\n",
    "    update_metrics_scores(\"Sequential Neural Network 2\", nn_predictions2)\n",
    "    postmodelling_render('Sequential Neural Network 2', fold, nn_predictions2)\n",
    "\n",
    "    # Convolutional Neural Network 1\n",
    "    # cmodel1 = create_cnn1_model()\n",
    "    # cmodel1.compile(optimizer='adam',\n",
    "    #     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    #     metrics=['accuracy'])\n",
    "    \n",
    "    # cmodel1.fit(train_data, train_labels, epochs=10)\n",
    "    # cnn_predictions1 = model1.predict(test_data)\n",
    "    # cnn_predictions1 = evaluate_predictions(cnn_predictions1)\n",
    "    # update_metrics_scores(\"Convolutional Neural Network 1\", cnn_predictions1)\n",
    "    # postmodelling_render('Convolutional Neural Network 1', fold, cnn_predictions1)\n",
    "\n",
    "    K.clear_session()\n",
    "\n",
    "for item in metrics_scores:\n",
    "    metrics_scores[item] = tuple(round(x / num_folds, 2) for x in metrics_scores[item])\n",
    "\n",
    "headers = [\"Model Name\", \"Accuracy\", \"Precision\", \"Recall\"]\n",
    "array_of_arrays = np.array([[key] + list(value) for key, value in metrics_scores.items()])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.axis(\"off\")\n",
    "table = ax.table(cellText=array_of_arrays, colLabels=headers, cellLoc=\"center\", loc=\"center\", colColours=[\"#f0f0f0\"] * len(headers))\n",
    "\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(10)\n",
    "\n",
    "table.auto_set_column_width([0, 1, 2, 3])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
