{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining notebook settings\n",
    "Configuration of the test design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Detection' or 'Classification' of erosion\n",
    "prediction_strategy = 'Detection'\n",
    "# 'raw', 'smoothed' or 'smoothed and alligned' scans PLY files must be used \n",
    "data_prep_stage = 'raw'\n",
    "\n",
    "# Train and test the models by stratified kfold\n",
    "perform_stratified_kfold = True\n",
    "# Train and test the models by file\n",
    "perform_kfold_byfile = True\n",
    "\n",
    "# Rendering boxplots for preliminary exploration is enabled\n",
    "boxplot_renders = True\n",
    "# Redering of scans before modelling is enabled\n",
    "premodeling_renders = True\n",
    "# Redering of scans after modelling is enabled\n",
    "postmodeling_renders = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to PLY files\n",
    "scandata_dir = 'scandata/' + data_prep_stage\n",
    "\n",
    "# Depth range for rendering\n",
    "min_renderdepth, max_renderdepth = {\n",
    "    'raw': (50, 400),\n",
    "    'smoothed': (-0.5, 0.5),\n",
    "    'smoothed and alligned': (-0.5, 0.5)\n",
    "}[data_prep_stage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotations for the erosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations = {\n",
    "    '221111_144114__binary_onlylines145900to148050.ply': {\n",
    "        (280, 560): 3,\n",
    "        (590, 640): 2,\n",
    "        (660, 700): 2,\n",
    "        (740, 810): 2,\n",
    "        (960, 1010): 2,\n",
    "        (1140, 1200): 2,\n",
    "        (1230, 1280): 2,\n",
    "        (1380, 1400): 2,\n",
    "        (1450, 1470): 2,\n",
    "    },\n",
    "    '221111_144114__binary_onlylines25660to28100.ply': {\n",
    "        (200, 530): 3,\n",
    "        (590, 660): 2,\n",
    "        (680, 740): 2,\n",
    "        (790, 880): 2,\n",
    "        (1080, 1110): 2,\n",
    "        (1270, 1330): 2,\n",
    "        (1380, 1430): 2,\n",
    "        (1560, 1580): 2,\n",
    "        (1660, 1700): 2,\n",
    "    },\n",
    "    '221111_144114__binary_onlylines47178to50517.ply': {\n",
    "        (2550, 3060): 3,\n",
    "        (2400, 2480): 2,\n",
    "        (2350, 2370): 2,\n",
    "        (2270, 2300): 2,\n",
    "        (1860, 1900): 2,\n",
    "        (1540, 1620): 2,\n",
    "        (1400, 1450): 2,\n",
    "        (1170, 1200): 2,\n",
    "        (1070, 1100): 2,\n",
    "    },'221111_144114__binary_onlylines86841to90142.ply': {\n",
    "        (240, 640): 3,\n",
    "        (730, 800): 2,\n",
    "        (840, 910): 2,\n",
    "        (950, 1080): 2,\n",
    "        (1340, 1420): 2,\n",
    "        (1670, 1740): 2,\n",
    "        (1820, 1890): 2,\n",
    "        (2130, 2170): 2,\n",
    "        (2240, 2270): 2,\n",
    "    },\n",
    "    '221111_144114__binary_onlylines114378to117271.ply': {\n",
    "        (2130, 2630): 3,\n",
    "        (1920, 2010): 2,\n",
    "        (1790, 1890): 2,\n",
    "        (1610, 1750): 2,\n",
    "        (1310, 1380): 2,\n",
    "        (1110, 1160): 2,\n",
    "        (1010, 1050): 2,\n",
    "        (860, 890): 2,\n",
    "        (770, 800): 2,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing necessary libraries\n",
    "\n",
    "These are third-party libraries that we used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from statistics import mean\n",
    "from loess import loess_1d\n",
    "\n",
    "import os\n",
    "import open3d as o3d\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "A function for rendering an anotation of erosion onto a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotation_line(ax, xmin, xmax, y, text, ytext = 0, linecolor = 'red', linewidth = 1, fontsize = 12):\n",
    "    ax.annotate('',\n",
    "                xy=(xmin, y),\n",
    "                xytext=(xmax, y),\n",
    "                xycoords='data',\n",
    "                textcoords='data',\n",
    "                arrowprops={'arrowstyle': '|-|',\n",
    "                            'color': linecolor, \n",
    "                            'linewidth':linewidth\n",
    "                           }\n",
    "               )\n",
    "    \n",
    "    ax.annotate('', \n",
    "                xy=(xmin, y), \n",
    "                xytext=(xmax, y), \n",
    "                xycoords='data', \n",
    "                textcoords='data',\n",
    "                arrowprops={'arrowstyle': '<->', \n",
    "                            'color': linecolor, \n",
    "                            'linewidth':linewidth\n",
    "                           }\n",
    "               )\n",
    "\n",
    "    xcenter = xmin + (xmax - xmin) / 2\n",
    "    \n",
    "    if ytext == 0:\n",
    "        ytext = y + (ax.get_ylim()[1] - ax.get_ylim()[0]) / 2\n",
    "\n",
    "    ax.annotate(text, xy=(xcenter,ytext), ha='center', va='center', fontsize=fontsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to pad to a list to given length with given values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_to_length_with(a, N, x = 0):\n",
    "    return a + [x] * (N - len(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to get errosion class at a given x coordinate in a given scan. Erosion classes are based on annotations (See above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_by_x(x, file_name):\n",
    "    for (start, end), label in annotations[file_name].items():\n",
    "        if x >= start and x <= end:\n",
    "            return (1 if prediction_strategy == \"Detection\" else label)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "Loading PLY files with `open3d`. `open3d` is optimized for handling `.ply` files efficiently. Takes around a minute. There are 3 coordinates (x, y and z) and 22.7 million rows in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PLY file names\n",
    "ply_file_names = os.listdir(scandata_dir)\n",
    "\n",
    "# Load PLY files\n",
    "ply_files = [os.path.join(scandata_dir, f) for f in ply_file_names if f.endswith(\".ply\")]\n",
    "\n",
    "# Load point clouds from PLY files and convert them to numpy arrays\n",
    "point_clouds = [np.asarray(o3d.io.read_point_cloud(ply_file).points).T for ply_file in ply_files]\n",
    "num_of_clouds = len(point_clouds)\n",
    "print(f\"Loaded {num_of_clouds} point clouds.\")\n",
    "\n",
    "total_rows = 0\n",
    "for i in range(num_of_clouds):\n",
    "    print(f\"Point cloud {ply_file_names[i]} shape: {point_clouds[i].shape}\")\n",
    "    total_rows += point_clouds[i].shape[1]\n",
    "print(f\"Total number of points: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The files have millions of rows indicating a missed measurement or no signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_missing_rows = 0\n",
    "for i in range(num_of_clouds):\n",
    "    print(f\"Rows with missing measurement in {ply_file_names[i]}: {np.sum(point_clouds[i][2] == 0)}\")\n",
    "    total_missing_rows += point_clouds[i].shape[1]\n",
    "print(f\"Total number of rows with missing measurement: {total_missing_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are measurements we want to delete. They don't have value to use and they will only act as (sever) noise to our machine learning models later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = 0\n",
    "for i in range(num_of_clouds):\n",
    "    # Find indices of points with missing z coordinate\n",
    "    z_zero_indices = np.where(point_clouds[i][2] == 0)[0]\n",
    "\n",
    "    # Delete points with missing measurment\n",
    "    point_clouds[i] = np.delete(point_clouds[i], z_zero_indices, axis=1)\n",
    "\n",
    "    print(f\"Point cloud {ply_file_names[i]} shape: {point_clouds[i].shape}\")\n",
    "    total_rows += point_clouds[i].shape[1]\n",
    "print(f\"Total number of rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Offesetting scans to start at 0 on X axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_of_clouds):\n",
    "    point_clouds[i][0] = point_clouds[i][0] - point_clouds[i][0].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary exploration\n",
    "\n",
    "Let's create a few plots to look at our data and how it is distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (boxplot_renders):\n",
    "    for i in range(num_of_clouds):\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(10,6))\n",
    "\n",
    "        fig.suptitle('Boxplots of ' + ply_file_names[i], fontsize=14)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "        axs[0].boxplot(point_clouds[i][0])\n",
    "        axs[0].set_title('X Axis')\n",
    "\n",
    "        axs[1].boxplot(point_clouds[i][1])\n",
    "        axs[1].set_title('Y Axis')\n",
    "\n",
    "        axs[2].boxplot(point_clouds[i][2])\n",
    "        axs[2].set_title('Z Axis')\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"Boxplot rendering is disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's render the pointclouds. Can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for each point cloud\n",
    "if(premodeling_renders):\n",
    "    fig, axes = plt.subplots(num_of_clouds, 1, figsize=(20, 5*num_of_clouds))\n",
    "\n",
    "    # If there's only one point cloud, wrap axes in a list\n",
    "    if num_of_clouds == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    # Loop through each point cloud and plot it in a separate subplot\n",
    "    for i, pointcloud in enumerate(point_clouds):\n",
    "        # Unpack x, y, z coordinates\n",
    "        x, y, z = pointcloud  \n",
    "        \n",
    "        # Scatter plot of the point cloud\n",
    "        sc = axes[i].scatter(x, y, c=z, s=0.1, vmin=min_renderdepth, vmax=max_renderdepth)\n",
    "        axes[i].set_title(ply_file_names[i])\n",
    "\n",
    "        # Adding annotations\n",
    "        if ply_file_names[i] in annotations:\n",
    "            for (start, end), label in annotations[ply_file_names[i]].items():\n",
    "                annotation_line(axes[i], start, end, 0, label, ytext = 10)\n",
    "\n",
    "        # Add colorbar for each subplot\n",
    "        cbar = plt.colorbar(sc, ax=axes[i])\n",
    "        cbar.set_label(\"Z Value (Color)\")\n",
    "\n",
    "    # Adjust layout for better spacing\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Rendering of scans before modelling is disabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Slicing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach we take is to divide the blade into slices of 10 lines. These slices, together with their corresponding erosion class are then individually fed to the machine learning algorithm to train it. After training is done, the algorithm returns a machine learning model. This model can be fed new slices, on which it will predict the erosion class. Since we use slices, we can only approximately say which part of the blade has eroded, because the drone does not record the exact x coordinate in space, but instead uses time while not flying at a constant speed.\n",
    "\n",
    "The slices should be 10 \"lines\" of the scanner large. Therefore, we can calculate the starting coordinates of each slice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_end_coordinates = []\n",
    "total_number_of_slices = 0\n",
    "\n",
    "for i in range(num_of_clouds):\n",
    "    pc_slice_end_coords = []\n",
    "    current_x = 0\n",
    "\n",
    "    for xi in np.unique(point_clouds[i][0])[1:]:\n",
    "        if xi >= current_x + 10:\n",
    "            current_x = xi\n",
    "            pc_slice_end_coords.append(xi)\n",
    "            \n",
    "    slice_end_coordinates.append(pc_slice_end_coords)\n",
    "    total_number_of_slices += len(pc_slice_end_coords)\n",
    "    print(\"There are\", len(pc_slice_end_coords), \"slices to be made from\", ply_file_names[i])\n",
    "    print(pc_slice_end_coords)\n",
    "    print(\"The total number of slices:\", total_number_of_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the start coordinates, we can actually make the slices. Note that we do create slices on the X and Y coordinate here, but we never use them afterwards. We will also \"lose\" one slice since the last starting coordinates are actually the end of the blade; there is nothing to be sliced after that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_point_clouds = []\n",
    "\n",
    "for i in range(num_of_clouds):\n",
    "    x, y, z = point_clouds[i]\n",
    "\n",
    "    x_slices = []\n",
    "    y_slices = []\n",
    "    z_slices = []\n",
    "\n",
    "    # end of the last slice\n",
    "    stop = slice_end_coordinates[i][-1]\n",
    "    # start of the first slice\n",
    "    slice_start = 0\n",
    "\n",
    "    for slice_end in slice_end_coordinates[i]:\n",
    "        if slice_start == stop:\n",
    "            break\n",
    "\n",
    "        mask = (x > slice_start) & (x <= slice_end)\n",
    "        x_slices.append(x[mask])\n",
    "        y_slices.append(y[mask])\n",
    "        z_slices.append(z[mask])\n",
    "        slice_start = slice_end\n",
    "\n",
    "    sliced_point_clouds.append((x_slices, y_slices, z_slices))\n",
    "\n",
    "    print(ply_file_names[i])\n",
    "    print(\"Least populated slice:\", min(len(z_slice) for z_slice in z_slices))\n",
    "    print(\"Most populated slice:\", max(len(z_slice) for z_slice in z_slices))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "Unfortunately, the slices have very different shapes. Some slices consist of as many as 16 thousand points, while others have only 2 thousand points. The machine learning algorithm cannot deal with this, so we will pad any slice with 0's until it has reached the maximum slice size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_slice_size = 0\n",
    "for i in range(num_of_clouds):\n",
    "    highest_slice_size = max(highest_slice_size, max(len(z_slice) for z_slice in sliced_point_clouds[i][2]))\n",
    "print(\"Most populated slice:\", highest_slice_size)\n",
    "\n",
    "padded_z_slices = []\n",
    "for i in range(num_of_clouds):\n",
    "    padded_z_slices.append([np.pad(z_slices, (0, highest_slice_size - len(z_slices)), 'constant', constant_values=(0, 0)) for z_slices in sliced_point_clouds[i][2]])\n",
    "\n",
    "print(len(padded_z_slices[0][0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labelling\n",
    "\n",
    "Just the last step before we can move on to machine learning: we need to label all the individual slices with their target value. This is the erosion class that corresponds with the slice. There is a helper function at the top of the file that helps us with this. The target values were determined manually based of the large graphics of us in the middle of this notebook.\n",
    "\n",
    "We use the mean to get +/- the center of the slice. 1 or 2 lines off is fine, since the slices measure 10 lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_labels = []\n",
    "\n",
    "# Labeling slices\n",
    "for i in range(num_of_clouds):\n",
    "    slice_labels.append([get_class_by_x(mean(x_slice), ply_file_names[i]) for x_slice in sliced_point_clouds[i][0]])\n",
    "    print(len(slice_labels[i]), \"slice labels from:\", ply_file_names[i])\n",
    "    print(slice_labels[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are 200 instances for each of the erosion categories 2 and 3. These are only about 15% of our data each, so we might encounter problems later on when train/test splitting. It may end up putting all of either category erosion cases in the train set, or the test set. That leads to an unrepresentative training set and will lower the usability of the resulting machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_labels = []\n",
    "total_counts = np.array([0,0] if prediction_strategy == \"Detection\" else [0,0,0])\n",
    "\n",
    "# Pie charts of label distribution\n",
    "fig, axes = plt.subplots(1, num_of_clouds + 1, figsize=(5 * (num_of_clouds + 1), 5))\n",
    "# Pie charts for each point cloud\n",
    "for i in range(num_of_clouds):\n",
    "    labels, counts = np.unique(slice_labels[i], return_counts=True)\n",
    "    axes[i].pie(counts, labels=labels, autopct='%1.1f%%', wedgeprops={'edgecolor': 'black'}, textprops={'fontsize': 14})\n",
    "    total_counts += np.array(counts)\n",
    "\n",
    "# Pie chart for all point clouds combined\n",
    "total_labels = np.unique(slice_labels[0])\n",
    "print(\"Total distribution of labels:\", total_counts)\n",
    "axes[-1].pie(total_counts, labels=labels, autopct='%1.1f%%', wedgeprops={'edgecolor': 'black'}, textprops={'fontsize': 14})\n",
    "axes[-1].set_title(\"Total\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = []\n",
    "all_filenames_and_slices = []\n",
    "\n",
    "for i in range(num_of_clouds):\n",
    "    all_labels += slice_labels[i]\n",
    "    all_filenames_and_slices += [(ply_file_names[i], padded_z_slices[i][j]) for j in range(len(slice_labels[i]))]\n",
    "\n",
    "all_labels = np.array(all_labels)\n",
    "all_filenames_and_slices = np.array(all_filenames_and_slices, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_from_indeces(train_indices, test_indices):\n",
    "    train_labels = all_labels[train_indices]\n",
    "    train_filenames, train_slices = zip(*all_filenames_and_slices[train_indices])\n",
    "    \n",
    "    test_labels = all_labels[test_indices]\n",
    "    test_filenames, test_slices = zip(*all_filenames_and_slices[test_indices])\n",
    "    \n",
    "    train = (train_labels, train_filenames, train_slices)\n",
    "    test = (test_labels, test_filenames, test_slices)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified K-fold\n",
    "20% of each class in each scan are combined and used for testing the remaining 80% of each class. This ensures that the proportion of each class in train and test sets remains the same as in the original dataset. However, in real world, an entire scan is only used either for training or for testing - something that K-fold by file achieves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (perform_stratified_kfold):\n",
    "    stratified_kfold = StratifiedKFold(shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_indices, test_indices) in enumerate(stratified_kfold.split(all_filenames_and_slices, all_labels)):\n",
    "        print(\"K-fold :\", fold + 1)\n",
    "\n",
    "        train, test = get_split_from_indeces(train_indices, test_indices)\n",
    "\n",
    "        stratified_train_labels, stratified_train_filenames, stratified_train_slices = train\n",
    "        stratified_test_labels, stratified_test_filenames, stratified_test_slices = test\n",
    "\n",
    "        unique_train_labels = np.unique(stratified_train_labels, return_counts=True)\n",
    "        unique_test_labels = np.unique(stratified_test_labels, return_counts=True)\n",
    "\n",
    "        print(\"\\t\",len(stratified_train_labels), \"training labels:\")\n",
    "        print(\"\\t\", unique_train_labels[0].tolist(), \"in quantities of\", unique_train_labels[1].tolist(), \"respectively.\")\n",
    "        print(\"\\t\",len(stratified_test_labels), \"training labels:\")\n",
    "        print(\"\\t\",unique_test_labels[0].tolist(), \"in quantities of\", unique_test_labels[1].tolist(), \"respectively.\")\n",
    "else:\n",
    "    print(\"Stratified kfold is disabled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold by file\n",
    "Each scan is used for testing models trained on other scans. Perfect representation can't be ensured, as class distribution sightly differs per scan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byfile_labels = []\n",
    "byfile_slices = []\n",
    "\n",
    "if (perform_kfold_byfile):\n",
    "    pass\n",
    "else:\n",
    "    print(\"Kfold by file is disabled.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
